{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olCj7elnvFN"
      },
      "source": [
        "# Legal Case Simulator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXZrBYt_mx97"
      },
      "source": [
        "# Get the data from the Legal Stories GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czwUvq_styuh"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "import os\n",
        "!pip install -q --upgrade torch\n",
        "!pip install -q transformers triton==3.4 kernels\n",
        "!pip uninstall -q torchvision torchaudio -y\n",
        "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
        "os.makedirs(\"/content/drive/MyDrive/hf_cache\", exist_ok=True)\n",
        "\n",
        "\n",
        "# # Get files\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/legal_doctrines_101.tsv\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/gpt3.5_story_question_101.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfapY-1impoH"
      },
      "outputs": [],
      "source": [
        "# Legal Doctrines 101 file\n",
        "import csv\n",
        "def load_doctrines():\n",
        "  # Format: concept\tintro_text\tword_count\n",
        "  with open('legal_doctrines_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "# GPT 3.5 Story Questions 101 file\n",
        "def load_story_questions():\n",
        "  # Format: concept\tintro_text\tstory\tconcept_question\tending_question\tlimitation_question\n",
        "  with open('gpt3.5_story_question_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "doctrines_data = load_doctrines()\n",
        "questions_data = load_story_questions()\n",
        "\n",
        "print(\"Doctrines Data: \")\n",
        "for row in doctrines_data:\n",
        "  print(row)\n",
        "\n",
        "print(\"\\nStory Questions Data: \")\n",
        "for row in questions_data:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for Finetuning"
      ],
      "metadata": {
        "id": "gsVXL-qVOQwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "def get_question_and_concept(question):\n",
        "  prompt =\"\"\n",
        "  completion = \"\"\n",
        "  prompt += \"Concept: \"\n",
        "  prompt += question[\"concept\"]\n",
        "  prompt += \". Summary Text: \"\n",
        "  prompt += question[\"intro_text\"]\n",
        "\n",
        "  completion += \"Story: \"\n",
        "  completion += question[\"story\"]\n",
        "  # May not need concept question for what we want to do, can be added if needed\n",
        "  # completion += \" Concept Question: \"\n",
        "  # completion += question[\"concept_question\"]\n",
        "  # completion += \" Limitation Question: \"\n",
        "  # completion += question[\"limitation_question\"]\n",
        "  completion += \" Question: \"\n",
        "  completion += question[\"ending_question\"]\n",
        "  completion_new = completion.replace(\"\\n\\n\", \"\\n\")\n",
        "  return prompt, completion_new\n",
        "\n",
        "\n",
        "def prepare_questions_finetuning_data(raw_data, filename='finetuning_data_questions.jsonl'):\n",
        "  finetuning_data = []\n",
        "  for question in raw_data:\n",
        "    data = {}\n",
        "    prompt, completion = get_question_and_concept(question)\n",
        "    data[\"messages\"] = [{\"role\":\"user\", \"content\":prompt}, {\"role\":\"assistant\", \"content\":completion}]\n",
        "    finetuning_data.append(data)\n",
        "  with open(filename, 'w') as out:\n",
        "    for data in finetuning_data:\n",
        "      out.write(json.dumps(data))\n",
        "      out.write('\\n')\n",
        "\n",
        "prepare_questions_finetuning_data(questions_data)\n"
      ],
      "metadata": {
        "id": "irdeFZMIOh8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print head of file\n",
        "!head finetuning_data_questions.jsonl"
      ],
      "metadata": {
        "id": "jaBhY2krO7Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune GPT3.5"
      ],
      "metadata": {
        "id": "cHzPzuMFxsBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import openai\n",
        "\n",
        "print(\"Enter openai api key: \")\n",
        "openai.api_key = input()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ],
      "metadata": {
        "id": "quFhcg_jxzg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "client.files.create(\n",
        "    file=open(\"finetuning_data_questions.jsonl\",'rb'),\n",
        "    purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "NFuzzz7vyNCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training File ID:')\n",
        "training_file = input()"
      ],
      "metadata": {
        "id": "Lz_8Z540yx6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=training_file,\n",
        "  model=\"gpt-3.5-turbo-0125\"\n",
        ")"
      ],
      "metadata": {
        "id": "1JQjmQjtzF_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a random topic for the case\n",
        "import random\n",
        "topic = random.choice(doctrines_data)\n",
        "\n",
        "# Create examples for the prompt\n",
        "ex1in, ex1out = get_question_and_concept(questions_data[0])\n",
        "ex2in, ex2out = get_question_and_concept(questions_data[1])\n",
        "ex3in, ex3out = get_question_and_concept(questions_data[2])\n",
        "story_so_far = None # Used to pass the previous question to prompt\n",
        "\n",
        "# Create the prompt\n",
        "# Used Chatgpt to improve the prompt to make the story continuous\n",
        "def initial_prompt():\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a legal case question generator that creates a multi-step case based on legal concepts. \"\n",
        "            \"You will produce a multiple-choice question built around a short narrative introduction. \"\n",
        "            \"The story must introduce characters, a location, and a situation that can be expanded later.\"\n",
        "            \"You MUST write in this structure:\\n\\n\"\n",
        "            \"1. STORY: A 3–6 sentence narrative introducing the situation.\\n\"\n",
        "            \"2. QUESTION: A single legal question about the concept.\\n\"\n",
        "            \"3. /ANSWER: The correct choice.\\n\"\n",
        "            \"4. RATIONALE: A short explanation.\\n\\n\"\n",
        "            \"Do NOT end the story — leave open threads so the next question can continue it.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Concept: {topic['concept']}. Summary Text: {topic['intro_text']}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def middle_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are continuing a legal case story. Your job is to write the NEXT question in the same narrative. \"\n",
        "            \"You MUST continue the same characters, same setting, same timeline, and the same story threads. \"\n",
        "            \"Use the previous story as canon and extend it naturally.\\n\\n\"\n",
        "            \"Your structure MUST be:\\n\"\n",
        "            \"1. STORY: 3–6 new sentences that continue the narrative.\\n\"\n",
        "            \"2. QUESTION: A new multiple-choice question about a NEW legal concept.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"Do not contradict earlier facts. Do not end the story — keep it open.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def end_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"Write the final question in this legal case. Continue the story from the previous section, \"\n",
        "            \"but this time bring the situation to a conclusion.\\n\\n\"\n",
        "            \"Structure:\\n\"\n",
        "            \"1. STORY: 3–6 sentences resolving the conflict.\\n\"\n",
        "            \"2. QUESTION: A final legal concept question.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"The story should conclude all major plot threads.\\n\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def generate_response(prompt, finetuned_model):\n",
        "  response = client.chat.completions.create(\n",
        "    model=finetuned_model,\n",
        "    messages=prompt,\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=[\"###\"]\n",
        "    )\n",
        "  return response.choices[0].message.content.strip()\n",
        "\n",
        "question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "print(\"BEGINNING: \")\n",
        "print(question)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far = question.split(\"/\")[0]\n",
        "question2 = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"MIDDLE: \")\n",
        "print(question2)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far += question2.split(\"/\")[0]\n",
        "question3 = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"END: \")\n",
        "print(question3)"
      ],
      "metadata": {
        "id": "zuQSl48R4W3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Reasoning Model"
      ],
      "metadata": {
        "id": "VD8Luo9jsSJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is your response? \")\n",
        "user_response = input()"
      ],
      "metadata": {
        "id": "xpBI6yI9gAce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using o3-mini, GptOss was way too big for colab to handle\n",
        "from openai import OpenAI\n",
        "client2 = OpenAI()\n",
        "\n",
        "o3mini_prompt = (\n",
        "    \"You are a legal evaluator. Given a question, the correct response, and the users response\"\n",
        "    \"return a score out of 4 points. \"\n",
        "    \"1 point for the correct letter answer.\"\n",
        "    \"1 point for mentioning the right doctrines/concept/keywords.\"\n",
        "    \"1 point for an okay justification or 2 points for a good justification.\"\n",
        "    \"Provide your resoning for the score. An example of how it should be formatted is below: \"\n",
        "    \"Example: \"\n",
        "    \"Score: 4\\n\"\n",
        "    \"Rationale: You provided the correct answer to the question, and mentioned the concept of legality.\"\n",
        "    \"Your justification could use some improvements though, it seemed somewhat ambiguous.\"\n",
        ")\n",
        "\n",
        "def evaluate_response(question, user_response, prompt, model):\n",
        "  response = client2.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\\nUser Response: {user_response}\"}\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")"
      ],
      "metadata": {
        "id": "U40qzgAJsd_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Users answer"
      ],
      "metadata": {
        "id": "lpMYrwLjfwdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_to_win = 6\n",
        "story_so_far = None\n",
        "user_response = \"\"\n",
        "win = False\n",
        "lose = False\n",
        "count = 0\n",
        "\n",
        "# Run game (use -1 to quit for now)\n",
        "while not win and not lose and user_response != \"-1\":\n",
        "  if count == 0:\n",
        "    # Start case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far = question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  elif count == questions_to_win:\n",
        "    # End case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  else:\n",
        "    # Middle case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far += question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "print(\"CASE CLOSED.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OTgj39VC7HAF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}